{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "222bbeaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: CPU random generator seem to be failing, disabling hardware random number generation\n",
      "WARNING: RDRND generated: 0xffffffff 0xffffffff 0xffffffff 0xffffffff\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from PIL import Image, ImageDraw\n",
    "import requests\n",
    "from pycocotools.coco import COCO\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from torch import nn, Tensor\n",
    "# from torchvision import transforms\n",
    "import torchvision\n",
    "# from torchvision.transforms import functional as F\n",
    "# from torchvision.transforms import transforms as T\n",
    "import mrcnn\n",
    "\n",
    "# Set the ROOT_DIR variable to the root directory of the Mask_RCNN git repo\n",
    "ROOT_DIR = './Mask_RCNN'\n",
    "assert os.path.exists(ROOT_DIR), 'ROOT_DIR does not exist. Did you forget to read the instructions above? ;)'\n",
    "\n",
    "# Import mrcnn libraries\n",
    "sys.path.append(ROOT_DIR) \n",
    "from mrcnn.config import Config\n",
    "# import mrcnn.utils as utils\n",
    "from mrcnn import visualize\n",
    "# import mrcnn.model as modellib\n",
    "from torchvision.ops import box_convert\n",
    "import albumentations\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e5f850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "# wandb.init(project=\"my-test-project\", entity=\"kolor200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff1c8869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "971ca534",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Inspired by\n",
    "# https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
    "\n",
    "# Composes transforms \n",
    "class Compose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "# List of transformations:\n",
    "\n",
    "# Transforms data into torch Tensors\n",
    "class ToTensor(nn.Module):\n",
    "    def forward(self, image, target):\n",
    "#         print(image.dtype, image.shape)\n",
    "        image = torchvision.transforms.functional.pil_to_tensor(image)\n",
    "        image = torchvision.transforms.functional.convert_image_dtype(image)\n",
    "#         image = torchvision.transforms.functional.convert_image_dtype(image)\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "# Not needed performed by transformation seen above\n",
    "class ConvertImageDtype(nn.Module):\n",
    "    def __init__(self, dtype) -> None:\n",
    "        super().__init__()\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def forward(self, image, target):\n",
    "        image = torchvision.transforms.functional.convert_image_dtype(image, self.dtype)\n",
    "        return image, target\n",
    "\n",
    "# Random Horizontal Flip\n",
    "class RandomHorizontalFlip(torchvision.transforms.transforms.RandomHorizontalFlip):\n",
    "    def forward(self, image, target):\n",
    "        if torch.rand(1) < self.p:\n",
    "            image = torchvision.transforms.functional.hflip(image)\n",
    "            if target is not None:\n",
    "                width, _ = torchvision.transforms.functional.get_image_size(image)\n",
    "                target[\"boxes\"][:, [0, 2]] = width - target[\"boxes\"][:, [2, 0]]\n",
    "                if \"masks\" in target:\n",
    "                    target[\"masks\"] = target[\"masks\"].flip(-1)\n",
    "        return image, target\n",
    "    \n",
    "\n",
    "class RandomImageEqualization(torchvision.transforms.transforms.RandomEqualize):\n",
    "    def forward(self, image, target):\n",
    "        if torch.rand(1) < self.p:\n",
    "            image = torchvision.transforms.functional.equalize(image)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class RandomGaussianBlur(torchvision.transforms.transforms.RandomEqualize):\n",
    "    def forward(self, image, target):\n",
    "        if torch.rand(1) < self.p:\n",
    "            image = torchvision.transforms.functional.gaussian_blur(image, kernel_size=(5, 9), sigma=(0.1, 5))\n",
    "        return image, target\n",
    "\n",
    "# This function applies random rotation, using wonderful library albumentations.ai\n",
    "class RandomRotation(nn.Module):\n",
    "    def __init__(self, p) -> None:\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        \n",
    "    def forward(self, image, target):\n",
    "        if torch.rand(1) < self.p:\n",
    "            \n",
    "            transform = albumentations.Compose([\n",
    "                albumentations.Rotate(p=0.5, limit=(-90, 90), mask_value=0)\n",
    "            ], bbox_params=albumentations.BboxParams(format='pascal_voc',  \n",
    "                                                     min_area=1024, \n",
    "                                                     min_visibility=0.1))\n",
    "              \n",
    "            image = np.asarray(image)\n",
    "            list_of_masks = []\n",
    "            for mask in target[\"masks\"]:\n",
    "                list_of_masks.append(np.asarray(mask))\n",
    "            bboxes = list(target[\"boxes\"])\n",
    "            \n",
    "            class_labels = list(np.asarray(target[\"labels\"]))\n",
    "            new_bboxes =[]\n",
    "            for bbox, class_label in zip(bboxes, class_labels):\n",
    "                bbox = list(np.asarray(bbox))\n",
    "                bbox.append(class_label)\n",
    "                new_bboxes.append(bbox)\n",
    "            \n",
    "            transformed = transform(image=image, masks=list_of_masks, bboxes=new_bboxes, class_labels=class_labels)\n",
    "            \n",
    "            transformed_image = transformed['image']\n",
    "            image = transformed_image\n",
    "            \n",
    "            transformed_bboxes = transformed['bboxes']\n",
    "            final_bboxes = []\n",
    "            for bbox in transformed_bboxes:\n",
    "                final_bboxes.append(bbox[:-1])\n",
    "            transformed_bboxes = torch.as_tensor(final_bboxes, dtype=torch.float32)\n",
    "            target[\"boxes\"] = transformed_bboxes\n",
    "            \n",
    "            transformed_masks = transformed['masks']\n",
    "            transformed_masks = np.dstack(transformed_masks)\n",
    "            transformed_masks = torch.as_tensor(np.copy(transformed_masks), dtype=torch.uint8)\n",
    "            transformed_masks = torch.permute(transformed_masks, (2,0,1))\n",
    "            target[\"masks\"] = transformed_masks\n",
    "            \n",
    "        return Image.fromarray(image), target\n",
    "    \n",
    "def get_train_transforms():\n",
    "    transforms = []\n",
    "#     transforms.append(RandomRotation(1))\n",
    "    transforms.append(ToTensor())\n",
    "    transforms.append(ConvertImageDtype(torch.uint8))\n",
    "    transforms.append(RandomHorizontalFlip(0.3))\n",
    "    transforms.append(RandomImageEqualization(0.2))\n",
    "    transforms.append(RandomGaussianBlur(0.2))\n",
    "    transforms.append(ConvertImageDtype(torch.float32))\n",
    "    return Compose(transforms)\n",
    "\n",
    "def get_val_transforms():\n",
    "    transforms = []\n",
    "    transforms.append(ToTensor())\n",
    "    return Compose(transforms)\n",
    "\n",
    "# dataset = LiveCellDataset(\"LiveCellDataset\", get_train_transforms())\n",
    "# image, target = dataset.__getitem__(14)\n",
    "# print(image.shape, image.dtype)\n",
    "# print(target['masks'].shape, target['masks'].dtype)\n",
    "# print('labels', target['labels'], target['labels'].dtype)\n",
    "# print('boxes:', target['boxes'].shape, target['boxes'].dtype) #correct\n",
    "# print('iscrowd:', target['iscrowd'], target['iscrowd'].dtype)\n",
    "# print('image_id:', target['image_id'], target['image_id'].dtype)\n",
    "# print('area:', target['area'], target['area'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a218bd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiveCellDataset(torch.utils.data.Dataset): #torch.utils.data.Dataset\n",
    "    def __init__(self, root, transforms, annotation_file_path, image_path_path):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        # Loading annotations here\n",
    "        #self.coco_annotation = COCO(annotation_file=\"../data/TrashCan/instances_train_trashcan.json\")\n",
    "        #self.coco_annotation = COCO(annotation_file=\"../data/TrashCan/instances_val_trashcan.json\")\n",
    "        self.coco_annotation = COCO(annotation_file=annotation_file_path)\n",
    "        self.image_path_path = image_path_path\n",
    "        \n",
    "        # Loading their ids, please note only one category!\n",
    "        self.image_ids = self.coco_annotation.getImgIds(catIds=[self.coco_annotation.getCatIds()[0]])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        img_info = self.coco_annotation.loadImgs([img_id])[0]\n",
    "        img_file_name = img_info[\"file_name\"]\n",
    "        \n",
    "        #img_path = Path(\"../data/TrashCan/train\") / img_file_name\n",
    "        #img_path = Path(\"../data/TrashCan/val\") / img_file_name\n",
    "        \n",
    "        img_path = Path(self.image_path_path) / img_file_name\n",
    "\n",
    "        \n",
    "        img = Image.open(str(img_path)).convert(\"RGB\")\n",
    "#         display_image(img)\n",
    "        \n",
    "        ann_ids = self.coco_annotation.getAnnIds(imgIds=[img_id], iscrowd=None)\n",
    "        anns = self.coco_annotation.loadAnns(ann_ids)\n",
    "        \n",
    "        target = self.get_target(anns, img_info, idx)\n",
    "    \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "            return img, target\n",
    "        else:\n",
    "            return ToTensor()(img, target)[0], target\n",
    "\n",
    "\n",
    "    def get_target(self, annotations, image_info, idx):\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        masks = []\n",
    "        area = []\n",
    "        iscrowd = []\n",
    "        \n",
    "        image_id = torch.tensor([idx], dtype=torch.int64)\n",
    "        \n",
    "        for annotation in annotations:\n",
    "            one_mask = self.coco_annotation.annToMask(annotation)\n",
    "            masks.append(one_mask)\n",
    "            \n",
    "            labels.append(annotation['category_id'])\n",
    "            \n",
    "            bounding_box = annotation['bbox']\n",
    "            boxes.append(bounding_box)\n",
    "            \n",
    "            _iscrowd = annotation['iscrowd']\n",
    "            iscrowd.append(_iscrowd)\n",
    "            \n",
    "            _area = annotation['area']\n",
    "            area.append(_area)\n",
    "            \n",
    "        masks = np.dstack(masks)\n",
    "        masks = torch.as_tensor(np.copy(masks), dtype=torch.uint8)\n",
    "        masks = torch.permute(masks, (2,0,1))\n",
    "        \n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "    \n",
    "        area = torch.as_tensor(area, dtype=torch.float64)\n",
    "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.uint8)\n",
    "        \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        boxes = box_convert(boxes, in_fmt='xywh', out_fmt='xyxy')\n",
    "        \n",
    "        assert len(boxes) == len(iscrowd) == len(labels) == len(area)\n",
    "        if not (len(boxes) == len(iscrowd) == len(labels) == len(area)):\n",
    "            print(len(boxes), len(iscrowd), len(labels), len(area))\n",
    "            \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        \n",
    "        return target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "# dataset = LiveCellDataset(\"LiveCellDataset\", get_val_transforms(), '../data/TrashCan/instances_train_trashcan.json', \n",
    "#                                 \"../data/TrashCan/train\")\n",
    "# image, target = dataset.__getitem__(15)\n",
    "# print(image.shape, image.dtype)\n",
    "# print(target['masks'].shape, target['masks'].dtype)\n",
    "# print('labels', target['labels'], target['labels'].dtype)\n",
    "# print('boxes:', target['boxes'].shape, target['boxes'].dtype) #correct\n",
    "# print('iscrowd:', target['iscrowd'], target['iscrowd'].dtype)\n",
    "# print('image_id:', target['image_id'], target['image_id'].dtype)\n",
    "# print('area:', target['area'], target['area'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d19821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source:\n",
    "# https://github.com/pytorch/vision/tree/main/references/segmentation\n",
    "import torch\n",
    "import torchvision.models.detection.mask_rcnn\n",
    "import utils_github.utils as utils\n",
    "from utils_github.coco_eval import CocoEvaluator\n",
    "from utils_github.coco_utils import get_coco_api_from_dataset\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq, scaler=None):\n",
    "    model.train()\n",
    "#     print(\"len(data_loader)/print_freq:\", len(data_loader)/print_freq)\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \", epoch=epoch, test=True, \n",
    "                                       how_much=int(len(data_loader)/print_freq))\n",
    "    metric_logger.add_meter(\"lr\", utils.SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n",
    "    header = f\"Epoch: [{epoch}]\"\n",
    "\n",
    "    lr_scheduler = None\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1.0 / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "\n",
    "        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
    "        )\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        # print(targets)\n",
    "        # print(targets.values)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "#             print(\"Moje\", losses)\n",
    "\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "#         print(\"Loss_dict_reduced\", loss_dict_reduced)\n",
    "#         print(\"Losses_reduced\", losses_reduced)\n",
    "        loss_value = losses_reduced.item()\n",
    "#         print(\"Loss_value\", loss_value)\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping training\")\n",
    "            print(loss_dict_reduced)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if scaler is not None:\n",
    "            scaler.scale(losses).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    return metric_logger\n",
    "\n",
    "\n",
    "def _get_iou_types(model):\n",
    "    model_without_ddp = model\n",
    "    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
    "        model_without_ddp = model.module\n",
    "    iou_types = [\"bbox\"]\n",
    "    if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):\n",
    "        iou_types.append(\"segm\")\n",
    "    if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN):\n",
    "        iou_types.append(\"keypoints\")\n",
    "    return iou_types\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate(model, data_loader, device, epoch):\n",
    "    n_threads = torch.get_num_threads()\n",
    "    # FIXME remove this and make paste_masks_in_image run on the GPU\n",
    "    torch.set_num_threads(1)\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    model.eval()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \", epoch=epoch, test=False)\n",
    "    header = \"Test:\"\n",
    "\n",
    "    coco = get_coco_api_from_dataset(data_loader.dataset)\n",
    "    iou_types = _get_iou_types(model)\n",
    "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, 100, header):\n",
    "        images = list(img.to(device) for img in images)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        model_time = time.time()\n",
    "        outputs = model(images)\n",
    "\n",
    "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "        model_time = time.time() - model_time\n",
    "\n",
    "        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
    "        evaluator_time = time.time()\n",
    "        coco_evaluator.update(res)\n",
    "        evaluator_time = time.time() - evaluator_time\n",
    "        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "#     print(\"Averaged stats:\", metric_logger)\n",
    "#     print(metric_logger.meters)\n",
    "#     wandb.log({\"epoch\": epoch,\"validation_loss\": metric_logger})\n",
    "    coco_evaluator.synchronize_between_processes()\n",
    "\n",
    "    # accumulate predictions from all images\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n",
    "    torch.set_num_threads(n_threads)\n",
    "    return coco_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23a5b235",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.21s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.19s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "dataset_train = LiveCellDataset(\"LiveCellDataset\", get_train_transforms(), \n",
    "                                '../data/TrashCan/instances_train_trashcan.json', \n",
    "                                \"../data/TrashCan/train\")\n",
    "\n",
    "dataset_test = LiveCellDataset(\"LiveCellDataset\", get_val_transforms(), \n",
    "'../data/TrashCan/instances_val_trashcan.json', \n",
    "\"../data/TrashCan/val\")\n",
    "\n",
    "# indices = torch.randperm(len(dataset_test)).tolist()\n",
    "# dataset_test = torch.utils.data.Subset(dataset_test, indices[:30])\n",
    "\n",
    "indices = torch.randperm(len(dataset_train)).tolist()\n",
    "dataset_train = torch.utils.data.Subset(dataset_train, indices[:2100])\n",
    "dataset_loader_train = torch.utils.data.DataLoader(\n",
    "dataset_train, batch_size=1, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
    "\n",
    "dataset_loader_test = torch.utils.data.DataLoader(\n",
    "dataset_test, batch_size=1, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
    "\n",
    "# image, target = dataset_test.__getitem__(15)\n",
    "# print(image.shape, image.dtype)\n",
    "# print(target['masks'].shape, target['masks'].dtype)\n",
    "# print('labels', target['labels'], target['labels'].dtype)\n",
    "# print('boxes:', target['boxes'].shape, target['boxes'].dtype) #correct\n",
    "# print('iscrowd:', target['iscrowd'], target['iscrowd'].dtype)\n",
    "# print('image_id:', target['image_id'], target['image_id'].dtype)\n",
    "# print('area:', target['area'], target['area'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "510f0772",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wandb.init(project=\"my-test-project\", entity=\"kolor200\")\n",
    "# Inspired by:\n",
    "# https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    # 1 + 22\n",
    "    num_classes = 23\n",
    "\n",
    "    model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                   step_size=3,\n",
    "                                                   gamma=0.1)\n",
    "    num_epochs = 10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_one_epoch(model, optimizer, dataset_loader_train, device, epoch, print_freq=10)\n",
    "        if epoch % 3 == 0:\n",
    "            print(\"Saving model\")\n",
    "            torch.save(model.state_dict(), f\"models/{epoch}.model\")\n",
    "        lr_scheduler.step()\n",
    "#         evaluate(model, dataset_loader_test, device=device, epoch=epoch)\n",
    "        \n",
    "    evaluate(model, dataset_loader_test, device=device, epoch=epoch)\n",
    "    torch.save(model.state_dict(), f\"models/{num_epochs}.model\")\n",
    "    return model \n",
    "#     print(\"That's it!\")\n",
    "# model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d25519f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8eb4de5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janek/anaconda3/envs/yolo6/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180544224/work/aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  [  0/524]  eta: 0:03:56  model_time: 0.3841 (0.3841)  evaluator_time: 0.0050 (0.0050)  time: 0.4518  data: 0.0619  max mem: 590\n",
      "Test:  [100/524]  eta: 0:01:18  model_time: 0.1655 (0.1752)  evaluator_time: 0.0056 (0.0062)  time: 0.1763  data: 0.0019  max mem: 590\n",
      "Test:  [200/524]  eta: 0:01:00  model_time: 0.1781 (0.1763)  evaluator_time: 0.0060 (0.0064)  time: 0.1897  data: 0.0019  max mem: 590\n",
      "Test:  [300/524]  eta: 0:00:41  model_time: 0.1708 (0.1755)  evaluator_time: 0.0056 (0.0063)  time: 0.1802  data: 0.0019  max mem: 590\n",
      "Test:  [400/524]  eta: 0:00:22  model_time: 0.1655 (0.1755)  evaluator_time: 0.0044 (0.0063)  time: 0.1800  data: 0.0017  max mem: 590\n",
      "Test:  [500/524]  eta: 0:00:04  model_time: 0.1774 (0.1748)  evaluator_time: 0.0057 (0.0062)  time: 0.1840  data: 0.0020  max mem: 590\n",
      "Test:  [523/524]  eta: 0:00:00  model_time: 0.1759 (0.1749)  evaluator_time: 0.0052 (0.0062)  time: 0.1854  data: 0.0019  max mem: 590\n",
      "Test: Total time: 0:01:36 (0.1836 s / it)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.21s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.20s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.286\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.453\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.323\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.279\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.274\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.315\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.369\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.424\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.424\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.379\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.420\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.434\n",
      "IoU metric: segm\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.235\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.431\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.225\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.221\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.228\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.301\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.315\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.347\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.347\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.321\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.365\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MaskRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=23, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=92, bias=True)\n",
       "    )\n",
       "    (mask_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(14, 14), sampling_ratio=2)\n",
       "    (mask_head): MaskRCNNHeads(\n",
       "      (mask_fcn1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (mask_fcn2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu2): ReLU(inplace=True)\n",
       "      (mask_fcn3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu3): ReLU(inplace=True)\n",
       "      (mask_fcn4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu4): ReLU(inplace=True)\n",
       "    )\n",
       "    (mask_predictor): MaskRCNNPredictor(\n",
       "      (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (mask_fcn_logits): Conv2d(256, 23, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model_instance_segmentation(23)\n",
    "model.load_state_dict(torch.load(\"models/10.model\"))\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "epoch = 0\n",
    "model.to(device)\n",
    "evaluate(model, dataset_loader_test, device=device, epoch=epoch)\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
