{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "222bbeaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: CPU random generator seem to be failing, disabling hardware random number generation\n",
      "WARNING: RDRND generated: 0xffffffff 0xffffffff 0xffffffff 0xffffffff\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from PIL import Image, ImageDraw\n",
    "import requests\n",
    "from pycocotools.coco import COCO\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from torch import nn, Tensor\n",
    "# from torchvision import transforms\n",
    "import torchvision\n",
    "# from torchvision.transforms import functional as F\n",
    "# from torchvision.transforms import transforms as T\n",
    "import mrcnn\n",
    "\n",
    "# Set the ROOT_DIR variable to the root directory of the Mask_RCNN git repo\n",
    "ROOT_DIR = './Mask_RCNN'\n",
    "assert os.path.exists(ROOT_DIR), 'ROOT_DIR does not exist. Did you forget to read the instructions above? ;)'\n",
    "\n",
    "# Import mrcnn libraries\n",
    "sys.path.append(ROOT_DIR) \n",
    "from mrcnn.config import Config\n",
    "# import mrcnn.utils as utils\n",
    "from mrcnn import visualize\n",
    "# import mrcnn.model as modellib\n",
    "from torchvision.ops import box_convert\n",
    "import albumentations\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e5f850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "# wandb.init(project=\"my-test-project\", entity=\"kolor200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff1c8869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "971ca534",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
    "\n",
    "# Composes transforms \n",
    "class Compose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "# List of transformations:\n",
    "\n",
    "# Transforms data into torch Tensors\n",
    "class ToTensor(nn.Module):\n",
    "    def forward(self, image, target):\n",
    "#         print(image.dtype, image.shape)\n",
    "        image = torchvision.transforms.functional.pil_to_tensor(image)\n",
    "        image = torchvision.transforms.functional.convert_image_dtype(image)\n",
    "#         image = torchvision.transforms.functional.convert_image_dtype(image)\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "# Not needed performed by transformation seen above\n",
    "class ConvertImageDtype(nn.Module):\n",
    "    def __init__(self, dtype) -> None:\n",
    "        super().__init__()\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def forward(self, image, target):\n",
    "        image = torchvision.transforms.functional.convert_image_dtype(image, self.dtype)\n",
    "        return image, target\n",
    "\n",
    "# Random Horizontal Flip\n",
    "class RandomHorizontalFlip(torchvision.transforms.transforms.RandomHorizontalFlip):\n",
    "    def forward(self, image, target):\n",
    "        if torch.rand(1) < self.p:\n",
    "            image = torchvision.transforms.functional.hflip(image)\n",
    "            if target is not None:\n",
    "                width, _ = torchvision.transforms.functional.get_image_size(image)\n",
    "                target[\"boxes\"][:, [0, 2]] = width - target[\"boxes\"][:, [2, 0]]\n",
    "                if \"masks\" in target:\n",
    "                    target[\"masks\"] = target[\"masks\"].flip(-1)\n",
    "        return image, target\n",
    "    \n",
    "\n",
    "class RandomImageEqualization(torchvision.transforms.transforms.RandomEqualize):\n",
    "    def forward(self, image, target):\n",
    "        if torch.rand(1) < self.p:\n",
    "            image = torchvision.transforms.functional.equalize(image)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class RandomGaussianBlur(torchvision.transforms.transforms.RandomEqualize):\n",
    "    def forward(self, image, target):\n",
    "        if torch.rand(1) < self.p:\n",
    "            image = torchvision.transforms.functional.gaussian_blur(image, kernel_size=(5, 9), sigma=(0.1, 5))\n",
    "        return image, target\n",
    "\n",
    "# This function applies random rotation, using wonderful library albumentations.ai\n",
    "class RandomRotation(nn.Module):\n",
    "    def __init__(self, p) -> None:\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        \n",
    "    def forward(self, image, target):\n",
    "        if torch.rand(1) < self.p:\n",
    "            \n",
    "            transform = albumentations.Compose([\n",
    "                albumentations.Rotate(p=0.5, limit=(-90, 90), mask_value=0)\n",
    "            ], bbox_params=albumentations.BboxParams(format='pascal_voc',  \n",
    "                                                     min_area=1024, \n",
    "                                                     min_visibility=0.1))\n",
    "              \n",
    "            image = np.asarray(image)\n",
    "            list_of_masks = []\n",
    "            for mask in target[\"masks\"]:\n",
    "                list_of_masks.append(np.asarray(mask))\n",
    "            bboxes = list(target[\"boxes\"])\n",
    "            \n",
    "            class_labels = list(np.asarray(target[\"labels\"]))\n",
    "            new_bboxes =[]\n",
    "            for bbox, class_label in zip(bboxes, class_labels):\n",
    "                bbox = list(np.asarray(bbox))\n",
    "                bbox.append(class_label)\n",
    "                new_bboxes.append(bbox)\n",
    "            \n",
    "            transformed = transform(image=image, masks=list_of_masks, bboxes=new_bboxes, class_labels=class_labels)\n",
    "            \n",
    "            transformed_image = transformed['image']\n",
    "            image = transformed_image\n",
    "            \n",
    "            transformed_bboxes = transformed['bboxes']\n",
    "            final_bboxes = []\n",
    "            for bbox in transformed_bboxes:\n",
    "                final_bboxes.append(bbox[:-1])\n",
    "            transformed_bboxes = torch.as_tensor(final_bboxes, dtype=torch.float32)\n",
    "            target[\"boxes\"] = transformed_bboxes\n",
    "            \n",
    "            transformed_masks = transformed['masks']\n",
    "            transformed_masks = np.dstack(transformed_masks)\n",
    "            transformed_masks = torch.as_tensor(np.copy(transformed_masks), dtype=torch.uint8)\n",
    "            transformed_masks = torch.permute(transformed_masks, (2,0,1))\n",
    "            target[\"masks\"] = transformed_masks\n",
    "            \n",
    "        return Image.fromarray(image), target\n",
    "    \n",
    "def get_train_transforms():\n",
    "    transforms = []\n",
    "#     transforms.append(RandomRotation(1))\n",
    "    transforms.append(ToTensor())\n",
    "    transforms.append(ConvertImageDtype(torch.uint8))\n",
    "    transforms.append(RandomHorizontalFlip(0.3))\n",
    "    transforms.append(RandomImageEqualization(0.2))\n",
    "    transforms.append(RandomGaussianBlur(0.2))\n",
    "    transforms.append(ConvertImageDtype(torch.float32))\n",
    "    return Compose(transforms)\n",
    "\n",
    "def get_val_transforms():\n",
    "    transforms = []\n",
    "    transforms.append(ToTensor())\n",
    "    return Compose(transforms)\n",
    "\n",
    "# dataset = LiveCellDataset(\"LiveCellDataset\", get_train_transforms())\n",
    "# image, target = dataset.__getitem__(14)\n",
    "# print(image.shape, image.dtype)\n",
    "# print(target['masks'].shape, target['masks'].dtype)\n",
    "# print('labels', target['labels'], target['labels'].dtype)\n",
    "# print('boxes:', target['boxes'].shape, target['boxes'].dtype) #correct\n",
    "# print('iscrowd:', target['iscrowd'], target['iscrowd'].dtype)\n",
    "# print('image_id:', target['image_id'], target['image_id'].dtype)\n",
    "# print('area:', target['area'], target['area'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a218bd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiveCellDataset(torch.utils.data.Dataset): #torch.utils.data.Dataset\n",
    "    def __init__(self, root, transforms, annotation_file_path, image_path_path):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        # Loading annotations here\n",
    "        #self.coco_annotation = COCO(annotation_file=\"../data/TrashCan/instances_train_trashcan.json\")\n",
    "        #self.coco_annotation = COCO(annotation_file=\"../data/TrashCan/instances_val_trashcan.json\")\n",
    "        self.coco_annotation = COCO(annotation_file=annotation_file_path)\n",
    "        self.image_path_path = image_path_path\n",
    "        \n",
    "        # Loading their ids, please note only one category!\n",
    "        self.image_ids = self.coco_annotation.getImgIds(catIds=[self.coco_annotation.getCatIds()[0]])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        img_info = self.coco_annotation.loadImgs([img_id])[0]\n",
    "        img_file_name = img_info[\"file_name\"]\n",
    "        \n",
    "        #img_path = Path(\"../data/TrashCan/train\") / img_file_name\n",
    "        #img_path = Path(\"../data/TrashCan/val\") / img_file_name\n",
    "        \n",
    "        img_path = Path(self.image_path_path) / img_file_name\n",
    "\n",
    "        \n",
    "        img = Image.open(str(img_path)).convert(\"RGB\")\n",
    "#         display_image(img)\n",
    "        \n",
    "        ann_ids = self.coco_annotation.getAnnIds(imgIds=[img_id], iscrowd=None)\n",
    "        anns = self.coco_annotation.loadAnns(ann_ids)\n",
    "        \n",
    "        target = self.get_target(anns, img_info, idx)\n",
    "    \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "            return img, target\n",
    "        else:\n",
    "            return ToTensor()(img, target)[0], target\n",
    "\n",
    "\n",
    "    def get_target(self, annotations, image_info, idx):\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        masks = []\n",
    "        area = []\n",
    "        iscrowd = []\n",
    "        \n",
    "        image_id = torch.tensor([idx], dtype=torch.int64)\n",
    "        \n",
    "        for annotation in annotations:\n",
    "            one_mask = self.coco_annotation.annToMask(annotation)\n",
    "            masks.append(one_mask)\n",
    "            \n",
    "            labels.append(annotation['category_id'])\n",
    "            \n",
    "            bounding_box = annotation['bbox']\n",
    "            boxes.append(bounding_box)\n",
    "            \n",
    "            _iscrowd = annotation['iscrowd']\n",
    "            iscrowd.append(_iscrowd)\n",
    "            \n",
    "            _area = annotation['area']\n",
    "            area.append(_area)\n",
    "            \n",
    "        masks = np.dstack(masks)\n",
    "        masks = torch.as_tensor(np.copy(masks), dtype=torch.uint8)\n",
    "        masks = torch.permute(masks, (2,0,1))\n",
    "        \n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "    \n",
    "        area = torch.as_tensor(area, dtype=torch.float64)\n",
    "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.uint8)\n",
    "        \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        boxes = box_convert(boxes, in_fmt='xywh', out_fmt='xyxy')\n",
    "        \n",
    "        assert len(boxes) == len(iscrowd) == len(labels) == len(area)\n",
    "        if not (len(boxes) == len(iscrowd) == len(labels) == len(area)):\n",
    "            print(len(boxes), len(iscrowd), len(labels), len(area))\n",
    "            \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        \n",
    "        return target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "# dataset = LiveCellDataset(\"LiveCellDataset\", get_val_transforms(), '../data/TrashCan/instances_train_trashcan.json', \n",
    "#                                 \"../data/TrashCan/train\")\n",
    "# image, target = dataset.__getitem__(15)\n",
    "# print(image.shape, image.dtype)\n",
    "# print(target['masks'].shape, target['masks'].dtype)\n",
    "# print('labels', target['labels'], target['labels'].dtype)\n",
    "# print('boxes:', target['boxes'].shape, target['boxes'].dtype) #correct\n",
    "# print('iscrowd:', target['iscrowd'], target['iscrowd'].dtype)\n",
    "# print('image_id:', target['image_id'], target['image_id'].dtype)\n",
    "# print('area:', target['area'], target['area'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d19821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source:\n",
    "# https://github.com/pytorch/vision/tree/main/references/segmentation\n",
    "import torch\n",
    "import torchvision.models.detection.mask_rcnn\n",
    "import utils_github.utils as utils\n",
    "from utils_github.coco_eval import CocoEvaluator\n",
    "from utils_github.coco_utils import get_coco_api_from_dataset\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq, scaler=None):\n",
    "    model.train()\n",
    "#     print(\"len(data_loader)/print_freq:\", len(data_loader)/print_freq)\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \", epoch=epoch, test=True, \n",
    "                                       how_much=int(len(data_loader)/print_freq))\n",
    "    metric_logger.add_meter(\"lr\", utils.SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n",
    "    header = f\"Epoch: [{epoch}]\"\n",
    "\n",
    "    lr_scheduler = None\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1.0 / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "\n",
    "        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
    "        )\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        # print(targets)\n",
    "        # print(targets.values)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "#             print(\"Moje\", losses)\n",
    "\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "#         print(\"Loss_dict_reduced\", loss_dict_reduced)\n",
    "#         print(\"Losses_reduced\", losses_reduced)\n",
    "        loss_value = losses_reduced.item()\n",
    "#         print(\"Loss_value\", loss_value)\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping training\")\n",
    "            print(loss_dict_reduced)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if scaler is not None:\n",
    "            scaler.scale(losses).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    return metric_logger\n",
    "\n",
    "\n",
    "def _get_iou_types(model):\n",
    "    model_without_ddp = model\n",
    "    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
    "        model_without_ddp = model.module\n",
    "    iou_types = [\"bbox\"]\n",
    "    if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):\n",
    "        iou_types.append(\"segm\")\n",
    "    if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN):\n",
    "        iou_types.append(\"keypoints\")\n",
    "    return iou_types\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate(model, data_loader, device, epoch):\n",
    "    n_threads = torch.get_num_threads()\n",
    "    # FIXME remove this and make paste_masks_in_image run on the GPU\n",
    "    torch.set_num_threads(1)\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    model.eval()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \", epoch=epoch, test=False)\n",
    "    header = \"Test:\"\n",
    "\n",
    "    coco = get_coco_api_from_dataset(data_loader.dataset)\n",
    "    iou_types = _get_iou_types(model)\n",
    "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, 100, header):\n",
    "        images = list(img.to(device) for img in images)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        model_time = time.time()\n",
    "        outputs = model(images)\n",
    "\n",
    "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "        model_time = time.time() - model_time\n",
    "\n",
    "        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
    "        evaluator_time = time.time()\n",
    "        coco_evaluator.update(res)\n",
    "        evaluator_time = time.time() - evaluator_time\n",
    "        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "#     print(\"Averaged stats:\", metric_logger)\n",
    "#     print(metric_logger.meters)\n",
    "#     wandb.log({\"epoch\": epoch,\"validation_loss\": metric_logger})\n",
    "    coco_evaluator.synchronize_between_processes()\n",
    "\n",
    "    # accumulate predictions from all images\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n",
    "    torch.set_num_threads(n_threads)\n",
    "    return coco_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23a5b235",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.20s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.19s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "dataset_train = LiveCellDataset(\"LiveCellDataset\", get_train_transforms(), \n",
    "                                '../data/TrashCan/instances_train_trashcan.json', \n",
    "                                \"../data/TrashCan/train\")\n",
    "\n",
    "dataset_test = LiveCellDataset(\"LiveCellDataset\", get_val_transforms(), \n",
    "'../data/TrashCan/instances_val_trashcan.json', \n",
    "\"../data/TrashCan/val\")\n",
    "\n",
    "indices = torch.randperm(len(dataset_test)).tolist()\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[:30])\n",
    "\n",
    "indices = torch.randperm(len(dataset_train)).tolist()\n",
    "dataset_train = torch.utils.data.Subset(dataset_train, indices[:2100])\n",
    "dataset_loader_train = torch.utils.data.DataLoader(\n",
    "dataset_train, batch_size=1, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
    "\n",
    "dataset_loader_test = torch.utils.data.DataLoader(\n",
    "dataset_test, batch_size=1, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
    "\n",
    "# image, target = dataset_test.__getitem__(15)\n",
    "# print(image.shape, image.dtype)\n",
    "# print(target['masks'].shape, target['masks'].dtype)\n",
    "# print('labels', target['labels'], target['labels'].dtype)\n",
    "# print('boxes:', target['boxes'].shape, target['boxes'].dtype) #correct\n",
    "# print('iscrowd:', target['iscrowd'], target['iscrowd'].dtype)\n",
    "# print('image_id:', target['image_id'], target['image_id'].dtype)\n",
    "# print('area:', target['area'], target['area'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "510f0772",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:306j3lrv) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 12596... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▇▅▃▃▂▂▃▃▂▂▂▂▂▁</td></tr><tr><td>loss_box_reg</td><td>▅▃▃█▅▂▂▄█▄▅▄▃▃▁</td></tr><tr><td>loss_classifier</td><td>█▇▄▂▂▁▁▁▂▁▂▁▁▁▁</td></tr><tr><td>loss_objectness</td><td>██▇▇▇▅▄▅▆▃▄▃▂▂▁</td></tr><tr><td>loss_rpn_box_reg</td><td>▃▁▁▂█▁▁▅▇▃▃▂▂▅▁</td></tr><tr><td>lr</td><td>▁▂▂▃▃▃▄▅▅▆▆▇▇██</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>0.18183</td></tr><tr><td>loss_box_reg</td><td>0.00088</td></tr><tr><td>loss_classifier</td><td>0.02634</td></tr><tr><td>loss_objectness</td><td>0.15412</td></tr><tr><td>loss_rpn_box_reg</td><td>0.00049</td></tr><tr><td>lr</td><td>0.00071</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">treasured-snowflake-34</strong>: <a href=\"https://wandb.ai/kolor200/my-test-project/runs/306j3lrv\" target=\"_blank\">https://wandb.ai/kolor200/my-test-project/runs/306j3lrv</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220126_200253-306j3lrv/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:306j3lrv). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kolor200/my-test-project/runs/2vzp8qhu\" target=\"_blank\">ancient-pine-35</a></strong> to <a href=\"https://wandb.ai/kolor200/my-test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch: [0]  [   0/2100]  eta: 0:16:00  lr: 0.000010  loss: 3.9903 (3.9903)  loss_classifier: 3.1872 (3.1872)  loss_box_reg: 0.0747 (0.0747)  loss_objectness: 0.7019 (0.7019)  loss_rpn_box_reg: 0.0266 (0.0266)  time: 0.4573  data: 0.1236  max mem: 3141\n",
      "Epoch: [0]  [  10/2100]  eta: 0:11:46  lr: 0.000060  loss: 3.9903 (4.0255)  loss_classifier: 3.1371 (3.0869)  loss_box_reg: 0.0567 (0.0710)  loss_objectness: 0.6929 (0.6956)  loss_rpn_box_reg: 0.0701 (0.1720)  time: 0.3380  data: 0.0128  max mem: 3943\n",
      "Epoch: [0]  [  20/2100]  eta: 0:11:27  lr: 0.000110  loss: 3.5413 (3.5532)  loss_classifier: 2.7859 (2.6706)  loss_box_reg: 0.0554 (0.0661)  loss_objectness: 0.6879 (0.6855)  loss_rpn_box_reg: 0.0651 (0.1310)  time: 0.3241  data: 0.0017  max mem: 3943\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12566/3663278042.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_12566/3663278042.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# train for one epoch, printing every 10 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_loader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saving model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12566/2348474351.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq, scaler)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yolo6/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yolo6/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"my-test-project\", entity=\"kolor200\")\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "    backbone.out_channels = 1280\n",
    "    anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                       aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                                    output_size=7,\n",
    "                                                    sampling_ratio=2)\n",
    "    return FasterRCNN(backbone,\n",
    "                       num_classes=num_classes,\n",
    "                       rpn_anchor_generator=anchor_generator,\n",
    "                       box_roi_pool=roi_pooler)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # train on the GPU or on the CPU, if a GPU is not available\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    # our dataset has two classes only - background and person\n",
    "    # 1 + 22\n",
    "    num_classes = 23\n",
    "\n",
    "    # get the model using our helper function\n",
    "    model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "    # move model to the right device\n",
    "    model.to(device)\n",
    "\n",
    "    # construct an optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "    # and a learning rate scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                   step_size=3,\n",
    "                                                   gamma=0.1)\n",
    "    # let's train it for 10 epochs\n",
    "    num_epochs = 10\n",
    "    print(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        # train for one epoch, printing every 10 iterations\n",
    "        train_one_epoch(model, optimizer, dataset_loader_train, device, epoch, print_freq=10)\n",
    "        if epoch % 3 == 0:\n",
    "            print(\"Saving model\")\n",
    "            torch.save(model.state_dict(), f\"models2/{epoch}.model\")\n",
    "            evaluate(model, dataset_loader_test, device=device, epoch=epoch)\n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        # evaluate on the test dataset\n",
    "#         evaluate(model, dataset_loader_test, device=device, epoch=epoch)\n",
    "        \n",
    "    \n",
    "    torch.save(model.state_dict(), f\"models2/{num_epochs}.model\")\n",
    "    return model \n",
    "\n",
    "model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25519f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935aa096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_model_instance_segmentation(23)\n",
    "# model.load_state_dict(torch.load(\"models/10.model\"))\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# epoch = 0\n",
    "# model.to(device)\n",
    "# evaluate(model, dataset_loader_test, device=device, epoch=epoch)\n",
    "# # model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
