{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "222bbeaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: CPU random generator seem to be failing, disabling hardware random number generation\n",
      "WARNING: RDRND generated: 0xffffffff 0xffffffff 0xffffffff 0xffffffff\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from PIL import Image, ImageDraw\n",
    "import requests\n",
    "from pycocotools.coco import COCO\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from torch import nn, Tensor\n",
    "# from torchvision import transforms\n",
    "import torchvision\n",
    "# from torchvision.transforms import functional as F\n",
    "# from torchvision.transforms import transforms as T\n",
    "import mrcnn\n",
    "\n",
    "# Set the ROOT_DIR variable to the root directory of the Mask_RCNN git repo\n",
    "ROOT_DIR = './Mask_RCNN'\n",
    "assert os.path.exists(ROOT_DIR), 'ROOT_DIR does not exist. Did you forget to read the instructions above? ;)'\n",
    "\n",
    "# Import mrcnn libraries\n",
    "sys.path.append(ROOT_DIR) \n",
    "from mrcnn.config import Config\n",
    "# import mrcnn.utils as utils\n",
    "from mrcnn import visualize\n",
    "# import mrcnn.model as modellib\n",
    "from torchvision.ops import box_convert\n",
    "import albumentations\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e5f850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "# wandb.init(project=\"my-test-project\", entity=\"kolor200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff1c8869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "971ca534",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
    "\n",
    "# Composes transforms \n",
    "class Compose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "# List of transformations:\n",
    "\n",
    "# Transforms data into torch Tensors\n",
    "class ToTensor(nn.Module):\n",
    "    def forward(self, image, target):\n",
    "#         print(image.dtype, image.shape)\n",
    "        image = torchvision.transforms.functional.pil_to_tensor(image)\n",
    "        image = torchvision.transforms.functional.convert_image_dtype(image)\n",
    "#         image = torchvision.transforms.functional.convert_image_dtype(image)\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "# Not needed performed by transformation seen above\n",
    "class ConvertImageDtype(nn.Module):\n",
    "    def __init__(self, dtype) -> None:\n",
    "        super().__init__()\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def forward(self, image, target):\n",
    "        image = torchvision.transforms.functional.convert_image_dtype(image, self.dtype)\n",
    "        return image, target\n",
    "\n",
    "# Random Horizontal Flip\n",
    "class RandomHorizontalFlip(torchvision.transforms.transforms.RandomHorizontalFlip):\n",
    "    def forward(self, image, target):\n",
    "        if torch.rand(1) < self.p:\n",
    "            image = torchvision.transforms.functional.hflip(image)\n",
    "            if target is not None:\n",
    "                width, _ = torchvision.transforms.functional.get_image_size(image)\n",
    "                target[\"boxes\"][:, [0, 2]] = width - target[\"boxes\"][:, [2, 0]]\n",
    "                if \"masks\" in target:\n",
    "                    target[\"masks\"] = target[\"masks\"].flip(-1)\n",
    "        return image, target\n",
    "    \n",
    "\n",
    "class RandomImageEqualization(torchvision.transforms.transforms.RandomEqualize):\n",
    "    def forward(self, image, target):\n",
    "        if torch.rand(1) < self.p:\n",
    "            image = torchvision.transforms.functional.equalize(image)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class RandomGaussianBlur(torchvision.transforms.transforms.RandomEqualize):\n",
    "    def forward(self, image, target):\n",
    "        if torch.rand(1) < self.p:\n",
    "            image = torchvision.transforms.functional.gaussian_blur(image, kernel_size=(5, 9), sigma=(0.1, 5))\n",
    "        return image, target\n",
    "\n",
    "# This function applies random rotation, using wonderful library albumentations.ai\n",
    "class RandomRotation(nn.Module):\n",
    "    def __init__(self, p) -> None:\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        \n",
    "    def forward(self, image, target):\n",
    "        if torch.rand(1) < self.p:\n",
    "            \n",
    "            transform = albumentations.Compose([\n",
    "                albumentations.Rotate(p=0.5, limit=(-90, 90), mask_value=0)\n",
    "            ], bbox_params=albumentations.BboxParams(format='pascal_voc',  \n",
    "                                                     min_area=1024, \n",
    "                                                     min_visibility=0.1))\n",
    "              \n",
    "            image = np.asarray(image)\n",
    "            list_of_masks = []\n",
    "            for mask in target[\"masks\"]:\n",
    "                list_of_masks.append(np.asarray(mask))\n",
    "            bboxes = list(target[\"boxes\"])\n",
    "            \n",
    "            class_labels = list(np.asarray(target[\"labels\"]))\n",
    "            new_bboxes =[]\n",
    "            for bbox, class_label in zip(bboxes, class_labels):\n",
    "                bbox = list(np.asarray(bbox))\n",
    "                bbox.append(class_label)\n",
    "                new_bboxes.append(bbox)\n",
    "            \n",
    "            transformed = transform(image=image, masks=list_of_masks, bboxes=new_bboxes, class_labels=class_labels)\n",
    "            \n",
    "            transformed_image = transformed['image']\n",
    "            image = transformed_image\n",
    "            \n",
    "            transformed_bboxes = transformed['bboxes']\n",
    "            final_bboxes = []\n",
    "            for bbox in transformed_bboxes:\n",
    "                final_bboxes.append(bbox[:-1])\n",
    "            transformed_bboxes = torch.as_tensor(final_bboxes, dtype=torch.float32)\n",
    "            target[\"boxes\"] = transformed_bboxes\n",
    "            \n",
    "            transformed_masks = transformed['masks']\n",
    "            transformed_masks = np.dstack(transformed_masks)\n",
    "            transformed_masks = torch.as_tensor(np.copy(transformed_masks), dtype=torch.uint8)\n",
    "            transformed_masks = torch.permute(transformed_masks, (2,0,1))\n",
    "            target[\"masks\"] = transformed_masks\n",
    "            \n",
    "        return Image.fromarray(image), target\n",
    "    \n",
    "def get_train_transforms():\n",
    "    transforms = []\n",
    "#     transforms.append(RandomRotation(1))\n",
    "    transforms.append(ToTensor())\n",
    "    transforms.append(ConvertImageDtype(torch.uint8))\n",
    "    transforms.append(RandomHorizontalFlip(0.3))\n",
    "    transforms.append(RandomImageEqualization(0.2))\n",
    "    transforms.append(RandomGaussianBlur(0.2))\n",
    "    transforms.append(ConvertImageDtype(torch.float32))\n",
    "    return Compose(transforms)\n",
    "\n",
    "def get_val_transforms():\n",
    "    transforms = []\n",
    "    transforms.append(ToTensor())\n",
    "    return Compose(transforms)\n",
    "\n",
    "# dataset = LiveCellDataset(\"LiveCellDataset\", get_train_transforms())\n",
    "# image, target = dataset.__getitem__(14)\n",
    "# print(image.shape, image.dtype)\n",
    "# print(target['masks'].shape, target['masks'].dtype)\n",
    "# print('labels', target['labels'], target['labels'].dtype)\n",
    "# print('boxes:', target['boxes'].shape, target['boxes'].dtype) #correct\n",
    "# print('iscrowd:', target['iscrowd'], target['iscrowd'].dtype)\n",
    "# print('image_id:', target['image_id'], target['image_id'].dtype)\n",
    "# print('area:', target['area'], target['area'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a218bd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiveCellDataset(torch.utils.data.Dataset): #torch.utils.data.Dataset\n",
    "    def __init__(self, root, transforms, annotation_file_path, image_path_path):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        # Loading annotations here\n",
    "        #self.coco_annotation = COCO(annotation_file=\"../data/TrashCan/instances_train_trashcan.json\")\n",
    "        #self.coco_annotation = COCO(annotation_file=\"../data/TrashCan/instances_val_trashcan.json\")\n",
    "        self.coco_annotation = COCO(annotation_file=annotation_file_path)\n",
    "        self.image_path_path = image_path_path\n",
    "        \n",
    "        # Loading their ids, please note only one category!\n",
    "        self.image_ids = self.coco_annotation.getImgIds(catIds=[self.coco_annotation.getCatIds()[0]])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        img_info = self.coco_annotation.loadImgs([img_id])[0]\n",
    "        img_file_name = img_info[\"file_name\"]\n",
    "        \n",
    "        #img_path = Path(\"../data/TrashCan/train\") / img_file_name\n",
    "        #img_path = Path(\"../data/TrashCan/val\") / img_file_name\n",
    "        \n",
    "        img_path = Path(self.image_path_path) / img_file_name\n",
    "\n",
    "        \n",
    "        img = Image.open(str(img_path)).convert(\"RGB\")\n",
    "#         display_image(img)\n",
    "        \n",
    "        ann_ids = self.coco_annotation.getAnnIds(imgIds=[img_id], iscrowd=None)\n",
    "        anns = self.coco_annotation.loadAnns(ann_ids)\n",
    "        \n",
    "        target = self.get_target(anns, img_info, idx)\n",
    "    \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "            return img, target\n",
    "        else:\n",
    "            return ToTensor()(img, target)[0], target\n",
    "\n",
    "\n",
    "    def get_target(self, annotations, image_info, idx):\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        masks = []\n",
    "        area = []\n",
    "        iscrowd = []\n",
    "        \n",
    "        image_id = torch.tensor([idx], dtype=torch.int64)\n",
    "        \n",
    "        for annotation in annotations:\n",
    "            one_mask = self.coco_annotation.annToMask(annotation)\n",
    "            masks.append(one_mask)\n",
    "            \n",
    "            labels.append(annotation['category_id'])\n",
    "            \n",
    "            bounding_box = annotation['bbox']\n",
    "            boxes.append(bounding_box)\n",
    "            \n",
    "            _iscrowd = annotation['iscrowd']\n",
    "            iscrowd.append(_iscrowd)\n",
    "            \n",
    "            _area = annotation['area']\n",
    "            area.append(_area)\n",
    "            \n",
    "        masks = np.dstack(masks)\n",
    "        masks = torch.as_tensor(np.copy(masks), dtype=torch.uint8)\n",
    "        masks = torch.permute(masks, (2,0,1))\n",
    "        \n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "    \n",
    "        area = torch.as_tensor(area, dtype=torch.float64)\n",
    "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.uint8)\n",
    "        \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        boxes = box_convert(boxes, in_fmt='xywh', out_fmt='xyxy')\n",
    "        \n",
    "        assert len(boxes) == len(iscrowd) == len(labels) == len(area)\n",
    "        if not (len(boxes) == len(iscrowd) == len(labels) == len(area)):\n",
    "            print(len(boxes), len(iscrowd), len(labels), len(area))\n",
    "            \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        \n",
    "        return target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "# dataset = LiveCellDataset(\"LiveCellDataset\", get_val_transforms(), '../data/TrashCan/instances_train_trashcan.json', \n",
    "#                                 \"../data/TrashCan/train\")\n",
    "# image, target = dataset.__getitem__(15)\n",
    "# print(image.shape, image.dtype)\n",
    "# print(target['masks'].shape, target['masks'].dtype)\n",
    "# print('labels', target['labels'], target['labels'].dtype)\n",
    "# print('boxes:', target['boxes'].shape, target['boxes'].dtype) #correct\n",
    "# print('iscrowd:', target['iscrowd'], target['iscrowd'].dtype)\n",
    "# print('image_id:', target['image_id'], target['image_id'].dtype)\n",
    "# print('area:', target['area'], target['area'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d19821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source:\n",
    "# https://github.com/pytorch/vision/tree/main/references/segmentation\n",
    "import torch\n",
    "import torchvision.models.detection.mask_rcnn\n",
    "import utils_github.utils as utils\n",
    "from utils_github.coco_eval import CocoEvaluator\n",
    "from utils_github.coco_utils import get_coco_api_from_dataset\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq, scaler=None):\n",
    "    model.train()\n",
    "#     print(\"len(data_loader)/print_freq:\", len(data_loader)/print_freq)\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \", epoch=epoch, test=True, \n",
    "                                       how_much=int(len(data_loader)/print_freq))\n",
    "    metric_logger.add_meter(\"lr\", utils.SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n",
    "    header = f\"Epoch: [{epoch}]\"\n",
    "\n",
    "    lr_scheduler = None\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1.0 / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "\n",
    "        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
    "        )\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        # print(targets)\n",
    "        # print(targets.values)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "#             print(\"Moje\", losses)\n",
    "\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "#         print(\"Loss_dict_reduced\", loss_dict_reduced)\n",
    "#         print(\"Losses_reduced\", losses_reduced)\n",
    "        loss_value = losses_reduced.item()\n",
    "#         print(\"Loss_value\", loss_value)\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping training\")\n",
    "            print(loss_dict_reduced)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if scaler is not None:\n",
    "            scaler.scale(losses).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    return metric_logger\n",
    "\n",
    "\n",
    "def _get_iou_types(model):\n",
    "    model_without_ddp = model\n",
    "    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
    "        model_without_ddp = model.module\n",
    "    iou_types = [\"bbox\"]\n",
    "    if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):\n",
    "        iou_types.append(\"segm\")\n",
    "    if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN):\n",
    "        iou_types.append(\"keypoints\")\n",
    "    return iou_types\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate(model, data_loader, device, epoch):\n",
    "    n_threads = torch.get_num_threads()\n",
    "    # FIXME remove this and make paste_masks_in_image run on the GPU\n",
    "    torch.set_num_threads(1)\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    model.eval()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \", epoch=epoch, test=False)\n",
    "    header = \"Test:\"\n",
    "\n",
    "    coco = get_coco_api_from_dataset(data_loader.dataset)\n",
    "    iou_types = _get_iou_types(model)\n",
    "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, 100, header):\n",
    "        images = list(img.to(device) for img in images)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        model_time = time.time()\n",
    "        outputs = model(images)\n",
    "\n",
    "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "        model_time = time.time() - model_time\n",
    "\n",
    "        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
    "        evaluator_time = time.time()\n",
    "        coco_evaluator.update(res)\n",
    "        evaluator_time = time.time() - evaluator_time\n",
    "        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "#     print(\"Averaged stats:\", metric_logger)\n",
    "#     print(metric_logger.meters)\n",
    "#     wandb.log({\"epoch\": epoch,\"validation_loss\": metric_logger})\n",
    "    coco_evaluator.synchronize_between_processes()\n",
    "\n",
    "    # accumulate predictions from all images\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n",
    "    torch.set_num_threads(n_threads)\n",
    "    return coco_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23a5b235",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.20s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "dataset_train = LiveCellDataset(\"LiveCellDataset\", get_train_transforms(), \n",
    "                                '../data/TrashCan/instances_train_trashcan.json', \n",
    "                                \"../data/TrashCan/train\")\n",
    "\n",
    "dataset_test = LiveCellDataset(\"LiveCellDataset\", get_val_transforms(), \n",
    "'../data/TrashCan/instances_val_trashcan.json', \n",
    "\"../data/TrashCan/val\")\n",
    "\n",
    "# indices = torch.randperm(len(dataset_test)).tolist()\n",
    "# dataset_test = torch.utils.data.Subset(dataset_test, indices[:30])\n",
    "\n",
    "indices = torch.randperm(len(dataset_train)).tolist()\n",
    "dataset_train = torch.utils.data.Subset(dataset_train, indices[:2100])\n",
    "dataset_loader_train = torch.utils.data.DataLoader(\n",
    "dataset_train, batch_size=1, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
    "\n",
    "dataset_loader_test = torch.utils.data.DataLoader(\n",
    "dataset_test, batch_size=1, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
    "\n",
    "# image, target = dataset_test.__getitem__(15)\n",
    "# print(image.shape, image.dtype)\n",
    "# print(target['masks'].shape, target['masks'].dtype)\n",
    "# print('labels', target['labels'], target['labels'].dtype)\n",
    "# print('boxes:', target['boxes'].shape, target['boxes'].dtype) #correct\n",
    "# print('iscrowd:', target['iscrowd'], target['iscrowd'].dtype)\n",
    "# print('image_id:', target['image_id'], target['image_id'].dtype)\n",
    "# print('area:', target['area'], target['area'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "510f0772",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wandb.init(project=\"my-test-project\", entity=\"kolor200\")\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    # train on the GPU or on the CPU, if a GPU is not available\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    # our dataset has two classes only - background and person\n",
    "    # 1 + 22\n",
    "    num_classes = 23\n",
    "\n",
    "    # get the model using our helper function\n",
    "    model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "    # move model to the right device\n",
    "    model.to(device)\n",
    "\n",
    "    # construct an optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "    # and a learning rate scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                   step_size=3,\n",
    "                                                   gamma=0.1)\n",
    "    # let's train it for 10 epochs\n",
    "    num_epochs = 10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # train for one epoch, printing every 10 iterations\n",
    "        train_one_epoch(model, optimizer, dataset_loader_train, device, epoch, print_freq=10)\n",
    "        if epoch % 3 == 0:\n",
    "            print(\"Saving model\")\n",
    "            torch.save(model.state_dict(), f\"models/{epoch}.model\")\n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        # evaluate on the test dataset\n",
    "#         evaluate(model, dataset_loader_test, device=device, epoch=epoch)\n",
    "        \n",
    "    evaluate(model, dataset_loader_test, device=device, epoch=epoch)\n",
    "    torch.save(model.state_dict(), f\"models/{num_epochs}.model\")\n",
    "    return model \n",
    "#     print(\"That's it!\")\n",
    "# model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25519f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8eb4de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n",
      "Test:  [  0/524]  eta: 0:02:42  model_time: 0.1772 (0.1772)  evaluator_time: 0.0050 (0.0050)  time: 0.3100  data: 0.1271  max mem: 601\n",
      "Test:  [100/524]  eta: 0:01:19  model_time: 0.1769 (0.1780)  evaluator_time: 0.0040 (0.0061)  time: 0.1854  data: 0.0018  max mem: 601\n",
      "Test:  [200/524]  eta: 0:01:00  model_time: 0.1668 (0.1764)  evaluator_time: 0.0047 (0.0061)  time: 0.1771  data: 0.0019  max mem: 601\n",
      "Test:  [300/524]  eta: 0:00:41  model_time: 0.1800 (0.1765)  evaluator_time: 0.0064 (0.0062)  time: 0.1903  data: 0.0018  max mem: 601\n",
      "Test:  [400/524]  eta: 0:00:22  model_time: 0.1796 (0.1758)  evaluator_time: 0.0057 (0.0063)  time: 0.1891  data: 0.0018  max mem: 601\n",
      "Test:  [500/524]  eta: 0:00:04  model_time: 0.1792 (0.1760)  evaluator_time: 0.0053 (0.0062)  time: 0.1872  data: 0.0019  max mem: 601\n",
      "Test:  [523/524]  eta: 0:00:00  model_time: 0.1786 (0.1762)  evaluator_time: 0.0053 (0.0062)  time: 0.1890  data: 0.0019  max mem: 601\n",
      "Test: Total time: 0:01:36 (0.1850 s / it)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.21s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.20s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.286\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.453\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.323\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.279\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.274\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.315\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.369\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.424\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.424\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.379\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.420\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.434\n",
      "IoU metric: segm\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.235\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.431\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.225\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.221\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.228\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.301\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.315\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.347\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.347\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.321\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.365\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<utils_github.coco_eval.CocoEvaluator at 0x7fe07a173580>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model_instance_segmentation(23)\n",
    "model.load_state_dict(torch.load(\"models/10.model\"))\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "epoch = 0\n",
    "model.to(device)\n",
    "evaluate(model, dataset_loader_test, device=device, epoch=epoch)\n",
    "# model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
