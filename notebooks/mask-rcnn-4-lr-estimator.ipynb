{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "222bbeaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: CPU random generator seem to be failing, disabling hardware random number generation\n",
      "WARNING: RDRND generated: 0xffffffff 0xffffffff 0xffffffff 0xffffffff\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from PIL import Image, ImageDraw\n",
    "import requests\n",
    "from pycocotools.coco import COCO\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from torch import nn, Tensor\n",
    "# from torchvision import transforms\n",
    "import torchvision\n",
    "# from torchvision.transforms import functional as F\n",
    "# from torchvision.transforms import transforms as T\n",
    "import mrcnn\n",
    "\n",
    "# Set the ROOT_DIR variable to the root directory of the Mask_RCNN git repo\n",
    "ROOT_DIR = './Mask_RCNN'\n",
    "assert os.path.exists(ROOT_DIR), 'ROOT_DIR does not exist. Did you forget to read the instructions above? ;)'\n",
    "\n",
    "# Import mrcnn libraries\n",
    "sys.path.append(ROOT_DIR) \n",
    "from mrcnn.config import Config\n",
    "# import mrcnn.utils as utils\n",
    "from mrcnn import visualize\n",
    "# import mrcnn.model as modellib\n",
    "from torchvision.ops import box_convert\n",
    "import albumentations\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e5f850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "# wandb.init(project=\"my-test-project\", entity=\"kolor200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff1c8869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "971ca534",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
    "\n",
    "# Composes transforms \n",
    "class Compose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "# List of transformations:\n",
    "\n",
    "# Transforms data into torch Tensors\n",
    "class ToTensor(nn.Module):\n",
    "    def forward(self, image, target):\n",
    "#         print(image.dtype, image.shape)\n",
    "        image = torchvision.transforms.functional.pil_to_tensor(image)\n",
    "        image = torchvision.transforms.functional.convert_image_dtype(image)\n",
    "#         image = torchvision.transforms.functional.convert_image_dtype(image)\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "# Not needed performed by transformation seen above\n",
    "class ConvertImageDtype(nn.Module):\n",
    "    def __init__(self, dtype) -> None:\n",
    "        super().__init__()\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def forward(self, image, target):\n",
    "        image = torchvision.transforms.functional.convert_image_dtype(image, self.dtype)\n",
    "        return image, target\n",
    "\n",
    "# Random Horizontal Flip\n",
    "class RandomHorizontalFlip(torchvision.transforms.transforms.RandomHorizontalFlip):\n",
    "    def forward(self, image, target):\n",
    "        if torch.rand(1) < self.p:\n",
    "            image = torchvision.transforms.functional.hflip(image)\n",
    "            if target is not None:\n",
    "                width, _ = torchvision.transforms.functional.get_image_size(image)\n",
    "                target[\"boxes\"][:, [0, 2]] = width - target[\"boxes\"][:, [2, 0]]\n",
    "                if \"masks\" in target:\n",
    "                    target[\"masks\"] = target[\"masks\"].flip(-1)\n",
    "        return image, target\n",
    "    \n",
    "\n",
    "class RandomImageEqualization(torchvision.transforms.transforms.RandomEqualize):\n",
    "    def forward(self, image, target):\n",
    "        if torch.rand(1) < self.p:\n",
    "            image = torchvision.transforms.functional.equalize(image)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class RandomGaussianBlur(torchvision.transforms.transforms.RandomEqualize):\n",
    "    def forward(self, image, target):\n",
    "        if torch.rand(1) < self.p:\n",
    "            image = torchvision.transforms.functional.gaussian_blur(image, kernel_size=(5, 9), sigma=(0.1, 5))\n",
    "        return image, target\n",
    "\n",
    "# This function applies random rotation, using wonderful library albumentations.ai\n",
    "class RandomRotation(nn.Module):\n",
    "    def __init__(self, p) -> None:\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        \n",
    "    def forward(self, image, target):\n",
    "        if torch.rand(1) < self.p:\n",
    "            \n",
    "            transform = albumentations.Compose([\n",
    "                albumentations.Rotate(p=0.5, limit=(-90, 90), mask_value=0)\n",
    "            ], bbox_params=albumentations.BboxParams(format='pascal_voc',  \n",
    "                                                     min_area=1024, \n",
    "                                                     min_visibility=0.1))\n",
    "              \n",
    "            image = np.asarray(image)\n",
    "            list_of_masks = []\n",
    "            for mask in target[\"masks\"]:\n",
    "                list_of_masks.append(np.asarray(mask))\n",
    "            bboxes = list(target[\"boxes\"])\n",
    "            \n",
    "            class_labels = list(np.asarray(target[\"labels\"]))\n",
    "            new_bboxes =[]\n",
    "            for bbox, class_label in zip(bboxes, class_labels):\n",
    "                bbox = list(np.asarray(bbox))\n",
    "                bbox.append(class_label)\n",
    "                new_bboxes.append(bbox)\n",
    "            \n",
    "            transformed = transform(image=image, masks=list_of_masks, bboxes=new_bboxes, class_labels=class_labels)\n",
    "            \n",
    "            transformed_image = transformed['image']\n",
    "            image = transformed_image\n",
    "            \n",
    "            transformed_bboxes = transformed['bboxes']\n",
    "            final_bboxes = []\n",
    "            for bbox in transformed_bboxes:\n",
    "                final_bboxes.append(bbox[:-1])\n",
    "            transformed_bboxes = torch.as_tensor(final_bboxes, dtype=torch.float32)\n",
    "            target[\"boxes\"] = transformed_bboxes\n",
    "            \n",
    "            transformed_masks = transformed['masks']\n",
    "            transformed_masks = np.dstack(transformed_masks)\n",
    "            transformed_masks = torch.as_tensor(np.copy(transformed_masks), dtype=torch.uint8)\n",
    "            transformed_masks = torch.permute(transformed_masks, (2,0,1))\n",
    "            target[\"masks\"] = transformed_masks\n",
    "            \n",
    "        return Image.fromarray(image), target\n",
    "    \n",
    "def get_train_transforms():\n",
    "    transforms = []\n",
    "#     transforms.append(RandomRotation(1))\n",
    "    transforms.append(ToTensor())\n",
    "    transforms.append(ConvertImageDtype(torch.uint8))\n",
    "    transforms.append(RandomHorizontalFlip(0.3))\n",
    "    transforms.append(RandomImageEqualization(0.2))\n",
    "    transforms.append(RandomGaussianBlur(0.2))\n",
    "    transforms.append(ConvertImageDtype(torch.float32))\n",
    "    return Compose(transforms)\n",
    "\n",
    "def get_val_transforms():\n",
    "    transforms = []\n",
    "    transforms.append(ToTensor())\n",
    "    return Compose(transforms)\n",
    "\n",
    "# dataset = LiveCellDataset(\"LiveCellDataset\", get_train_transforms())\n",
    "# image, target = dataset.__getitem__(14)\n",
    "# print(image.shape, image.dtype)\n",
    "# print(target['masks'].shape, target['masks'].dtype)\n",
    "# print('labels', target['labels'], target['labels'].dtype)\n",
    "# print('boxes:', target['boxes'].shape, target['boxes'].dtype) #correct\n",
    "# print('iscrowd:', target['iscrowd'], target['iscrowd'].dtype)\n",
    "# print('image_id:', target['image_id'], target['image_id'].dtype)\n",
    "# print('area:', target['area'], target['area'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a218bd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiveCellDataset(torch.utils.data.Dataset): #torch.utils.data.Dataset\n",
    "    def __init__(self, root, transforms, annotation_file_path, image_path_path):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        # Loading annotations here\n",
    "        #self.coco_annotation = COCO(annotation_file=\"../data/TrashCan/instances_train_trashcan.json\")\n",
    "        #self.coco_annotation = COCO(annotation_file=\"../data/TrashCan/instances_val_trashcan.json\")\n",
    "        self.coco_annotation = COCO(annotation_file=annotation_file_path)\n",
    "        self.image_path_path = image_path_path\n",
    "        \n",
    "        # Loading their ids, please note only one category!\n",
    "        self.image_ids = self.coco_annotation.getImgIds(catIds=[self.coco_annotation.getCatIds()[0]])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        img_info = self.coco_annotation.loadImgs([img_id])[0]\n",
    "        img_file_name = img_info[\"file_name\"]\n",
    "        \n",
    "        #img_path = Path(\"../data/TrashCan/train\") / img_file_name\n",
    "        #img_path = Path(\"../data/TrashCan/val\") / img_file_name\n",
    "        \n",
    "        img_path = Path(self.image_path_path) / img_file_name\n",
    "\n",
    "        \n",
    "        img = Image.open(str(img_path)).convert(\"RGB\")\n",
    "#         display_image(img)\n",
    "        \n",
    "        ann_ids = self.coco_annotation.getAnnIds(imgIds=[img_id], iscrowd=None)\n",
    "        anns = self.coco_annotation.loadAnns(ann_ids)\n",
    "        \n",
    "        target = self.get_target(anns, img_info, idx)\n",
    "    \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "            return img, target\n",
    "        else:\n",
    "            return ToTensor()(img, target)[0], target\n",
    "\n",
    "\n",
    "    def get_target(self, annotations, image_info, idx):\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        masks = []\n",
    "        area = []\n",
    "        iscrowd = []\n",
    "        \n",
    "        image_id = torch.tensor([idx], dtype=torch.int64)\n",
    "        \n",
    "        for annotation in annotations:\n",
    "            one_mask = self.coco_annotation.annToMask(annotation)\n",
    "            masks.append(one_mask)\n",
    "            \n",
    "            labels.append(annotation['category_id'])\n",
    "            \n",
    "            bounding_box = annotation['bbox']\n",
    "            boxes.append(bounding_box)\n",
    "            \n",
    "            _iscrowd = annotation['iscrowd']\n",
    "            iscrowd.append(_iscrowd)\n",
    "            \n",
    "            _area = annotation['area']\n",
    "            area.append(_area)\n",
    "            \n",
    "        masks = np.dstack(masks)\n",
    "        masks = torch.as_tensor(np.copy(masks), dtype=torch.uint8)\n",
    "        masks = torch.permute(masks, (2,0,1))\n",
    "        \n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "    \n",
    "        area = torch.as_tensor(area, dtype=torch.float64)\n",
    "        iscrowd = torch.as_tensor(iscrowd, dtype=torch.uint8)\n",
    "        \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        boxes = box_convert(boxes, in_fmt='xywh', out_fmt='xyxy')\n",
    "        \n",
    "        assert len(boxes) == len(iscrowd) == len(labels) == len(area)\n",
    "        if not (len(boxes) == len(iscrowd) == len(labels) == len(area)):\n",
    "            print(len(boxes), len(iscrowd), len(labels), len(area))\n",
    "            \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        \n",
    "        return target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "# dataset = LiveCellDataset(\"LiveCellDataset\", get_val_transforms(), '../data/TrashCan/instances_train_trashcan.json', \n",
    "#                                 \"../data/TrashCan/train\")\n",
    "# image, target = dataset.__getitem__(15)\n",
    "# print(image.shape, image.dtype)\n",
    "# print(target['masks'].shape, target['masks'].dtype)\n",
    "# print('labels', target['labels'], target['labels'].dtype)\n",
    "# print('boxes:', target['boxes'].shape, target['boxes'].dtype) #correct\n",
    "# print('iscrowd:', target['iscrowd'], target['iscrowd'].dtype)\n",
    "# print('image_id:', target['image_id'], target['image_id'].dtype)\n",
    "# print('area:', target['area'], target['area'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d19821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source:\n",
    "# https://github.com/pytorch/vision/tree/main/references/segmentation\n",
    "import torch\n",
    "import torchvision.models.detection.mask_rcnn\n",
    "import utils_github.utils as utils\n",
    "from utils_github.coco_eval import CocoEvaluator\n",
    "from utils_github.coco_utils import get_coco_api_from_dataset\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq, scaler=None):\n",
    "    model.train()\n",
    "#     print(\"len(data_loader)/print_freq:\", len(data_loader)/print_freq)\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \", epoch=epoch, test=True, \n",
    "                                       how_much=int(len(data_loader)/print_freq))\n",
    "    metric_logger.add_meter(\"lr\", utils.SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n",
    "    header = f\"Epoch: [{epoch}]\"\n",
    "\n",
    "    lr_scheduler = None\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1.0 / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "\n",
    "        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
    "        )\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        # print(targets)\n",
    "        # print(targets.values)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "#             print(\"Moje\", losses)\n",
    "\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "#         print(\"Loss_dict_reduced\", loss_dict_reduced)\n",
    "#         print(\"Losses_reduced\", losses_reduced)\n",
    "        loss_value = losses_reduced.item()\n",
    "#         print(\"Loss_value\", loss_value)\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping training\")\n",
    "            print(loss_dict_reduced)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if scaler is not None:\n",
    "            scaler.scale(losses).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    return metric_logger\n",
    "\n",
    "\n",
    "def _get_iou_types(model):\n",
    "    model_without_ddp = model\n",
    "    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
    "        model_without_ddp = model.module\n",
    "    iou_types = [\"bbox\"]\n",
    "    if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):\n",
    "        iou_types.append(\"segm\")\n",
    "    if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN):\n",
    "        iou_types.append(\"keypoints\")\n",
    "    return iou_types\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate(model, data_loader, device, epoch):\n",
    "    n_threads = torch.get_num_threads()\n",
    "    # FIXME remove this and make paste_masks_in_image run on the GPU\n",
    "    torch.set_num_threads(1)\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    model.eval()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \", epoch=epoch, test=False)\n",
    "    header = \"Test:\"\n",
    "\n",
    "    coco = get_coco_api_from_dataset(data_loader.dataset)\n",
    "    iou_types = _get_iou_types(model)\n",
    "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, 100, header):\n",
    "        images = list(img.to(device) for img in images)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        model_time = time.time()\n",
    "        outputs = model(images)\n",
    "\n",
    "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "        model_time = time.time() - model_time\n",
    "\n",
    "        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
    "        evaluator_time = time.time()\n",
    "        coco_evaluator.update(res)\n",
    "        evaluator_time = time.time() - evaluator_time\n",
    "        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "#     print(\"Averaged stats:\", metric_logger)\n",
    "#     print(metric_logger.meters)\n",
    "#     wandb.log({\"epoch\": epoch,\"validation_loss\": metric_logger})\n",
    "    coco_evaluator.synchronize_between_processes()\n",
    "\n",
    "    # accumulate predictions from all images\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n",
    "    torch.set_num_threads(n_threads)\n",
    "    return coco_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23a5b235",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.20s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.19s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "dataset_train = LiveCellDataset(\"LiveCellDataset\", get_train_transforms(), \n",
    "                                '../data/TrashCan/instances_train_trashcan.json', \n",
    "                                \"../data/TrashCan/train\")\n",
    "\n",
    "dataset_test = LiveCellDataset(\"LiveCellDataset\", get_val_transforms(), \n",
    "'../data/TrashCan/instances_val_trashcan.json', \n",
    "\"../data/TrashCan/val\")\n",
    "\n",
    "indices = torch.randperm(len(dataset_test)).tolist()\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[:30])\n",
    "\n",
    "indices = torch.randperm(len(dataset_train)).tolist()\n",
    "dataset_train = torch.utils.data.Subset(dataset_train, indices[:2100])\n",
    "dataset_loader_train = torch.utils.data.DataLoader(\n",
    "dataset_train, batch_size=1, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
    "\n",
    "dataset_loader_test = torch.utils.data.DataLoader(\n",
    "dataset_test, batch_size=1, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
    "\n",
    "# image, target = dataset_test.__getitem__(15)\n",
    "# print(image.shape, image.dtype)\n",
    "# print(target['masks'].shape, target['masks'].dtype)\n",
    "# print('labels', target['labels'], target['labels'].dtype)\n",
    "# print('boxes:', target['boxes'].shape, target['boxes'].dtype) #correct\n",
    "# print('iscrowd:', target['iscrowd'], target['iscrowd'].dtype)\n",
    "# print('image_id:', target['image_id'], target['image_id'].dtype)\n",
    "# print('area:', target['area'], target['area'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "510f0772",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wandb.init(project=\"my-test-project\", entity=\"kolor200\")\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    # train on the GPU or on the CPU, if a GPU is not available\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    # our dataset has two classes only - background and person\n",
    "    # 1 + 22\n",
    "    num_classes = 23\n",
    "\n",
    "    # get the model using our helper function\n",
    "    model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "    # move model to the right device\n",
    "    model.to(device)\n",
    "\n",
    "    # construct an optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "    # and a learning rate scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                   step_size=3,\n",
    "                                                   gamma=0.1)\n",
    "    # let's train it for 10 epochs\n",
    "    num_epochs = 10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # train for one epoch, printing every 10 iterations\n",
    "        train_one_epoch(model, optimizer, dataset_loader_train, device, epoch, print_freq=10)\n",
    "        if epoch % 3 == 0:\n",
    "            print(\"Saving model\")\n",
    "            torch.save(model.state_dict(), f\"models/{epoch}.model\")\n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        # evaluate on the test dataset\n",
    "#         evaluate(model, dataset_loader_test, device=device, epoch=epoch)\n",
    "        \n",
    "    evaluate(model, dataset_loader_test, device=device, epoch=epoch)\n",
    "    torch.save(model.state_dict(), f\"models/{num_epochs}.model\")\n",
    "    return model \n",
    "#     print(\"That's it!\")\n",
    "# model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d25519f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da885364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd146813b0944021b62c08b9d276fdce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early, the loss has diverged\n",
      "Learning rate search finished. See the graph with {finder_name}.plot()\n",
      "LR suggestion: steepest gradient\n",
      "Suggested LR: 1.36E-02\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch_lr_finder import LRFinder\n",
    "model = get_model_instance_segmentation(23)\n",
    "model.eval()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\n",
    "lr_finder.range_test(dataset_loader_test, end_lr=100, num_iter=100)\n",
    "lr_finder.plot() # to inspect the loss-learning rate graph\n",
    "lr_finder.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
